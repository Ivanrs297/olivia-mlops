{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716d937e-5854-4900-8a9a-a01c08526aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:17<00:00, 65.86s/it]\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# # Load model\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"  #'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = \"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\"\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id, use_auth_token=hf_auth, cache_dir=\"./hub\"\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_auth,\n",
    "    cache_dir=\"./hub\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# # Load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id, use_auth_token=hf_auth, cache_dir=\"./hub\"\n",
    ")\n",
    "\n",
    "stop_list = [\"\\nHuman:\", \"\\n```\\n\"]\n",
    "\n",
    "stop_token_ids = [tokenizer(x)[\"input_ids\"] for x in stop_list]\n",
    "\n",
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
    "    ) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids) :], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task=\"text-generation\",\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    ")\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import re\n",
    "from olivia_questions import questions\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a9e35c-0b70-4fec-b5d3-3d7c7c1f23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import re\n",
    "from olivia_questions import questions\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1686a4de-3110-4d83-9ac9-1d9f6cafa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Basado en los siguientes fragmentos de una entrevista, responde la pregunta del final (si en los fragmentos no se encuentra la respuesta a la pregunta del final, responde 'NA').\n",
    "\n",
    "Fragmentos:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c81f05b4-4086-4807-aebe-8f5485fcc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update(audio_id):\n",
    "    # load document\n",
    "    filepath = f\"./transcriptions/{audio_id}.vtt\"\n",
    "    loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # load embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "    )\n",
    "\n",
    "    # clean documents\n",
    "    pattern = r\"\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}\\.\\d{3}\"\n",
    "    source_documents = []\n",
    "    for doc in documents:\n",
    "        doc.page_content = doc.page_content.replace(\"WEBVTT\", \"\")\n",
    "        doc.page_content = re.sub(pattern, \"\", doc.page_content)\n",
    "        doc.page_content = doc.page_content.replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "        source_documents.append(doc)\n",
    "\n",
    "    # split documents into texts\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
    "    texts = text_splitter.split_documents(source_documents)\n",
    "\n",
    "    # generate vectore store\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    # generate QA chain\n",
    "    prompt = PromptTemplate(\n",
    "        template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        # retriever=vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6}),\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        # verbose=True\n",
    "    )\n",
    "\n",
    "    # generate patch update\n",
    "    update = {}\n",
    "    for qn in questions:\n",
    "        query = qn[\"question\"]\n",
    "        print(\"Q:\", query)\n",
    "        answer = qa_chain.run(query)\n",
    "        answer = answer.split(\"\\n\")[0].strip()\n",
    "        if \"categories\" in qn.keys():\n",
    "            print(\"Available Categories\", qn[\"categories\"])\n",
    "            cats = FAISS.from_texts(qn[\"categories\"], embeddings)\n",
    "            scores = cats.similarity_search_with_score(answer)\n",
    "            highest = max(scores, key=lambda x: x[1])\n",
    "            if highest[1] < 0.5 and qn[\"otherKey\"] is not None:\n",
    "                update[qn[\"otherKey\"]] = answer\n",
    "                print(\"A:\", answer)\n",
    "            else:\n",
    "                update[qn[\"key\"]] = highest[0].page_content\n",
    "                print(\"A:\", highest[0].page_content)\n",
    "        elif \"type\" in qn.keys() and qn[\"type\"] == \"boolean\":\n",
    "            update[qn[\"key\"]] = len(answer) > 0\n",
    "            print(\"A:\", len(answer) > 0)\n",
    "        else:\n",
    "            update[qn[\"key\"]] = answer\n",
    "            print(\"A:\", answer)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return update\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def connect():\n",
    "    # change endpoint and data\n",
    "    # url = f\"http://localhost:8080/api/login\"\n",
    "    url = f\"https://api.olivia-fairlac.org/api/login\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    data = {\"email\": \"david@gmail.com\", \"password\": \"ol1v14\"}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        res = response.json()\n",
    "        token = res[\"token\"]\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_cedula_by_expediente_id(token, id):\n",
    "    url = f\"https://api.olivia-fairlac.org/api/cedula/{id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        audio_list = response.json()\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "    return audio_list\n",
    "\n",
    "\n",
    "def get_expediente_by_id(token, id):\n",
    "    # TODO: change endpoint\n",
    "    url = f\"https://api.olivia-fairlac.org/api/expediente/{id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        audio_list = response.json()\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "    return audio_list\n",
    "\n",
    "\n",
    "def get_expedientes(token):\n",
    "    # TODO: change endpoint\n",
    "    url = f\"https://api.olivia-fairlac.org/api/expediente\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    results = []\n",
    "    if response.status_code == 200:\n",
    "        items = response.json()[\"docs\"]\n",
    "        for item in items:\n",
    "            details = get_expediente_by_id(token, item[\"_id\"])\n",
    "            cedula = get_cedula_by_expediente_id(token, item[\"_id\"])\n",
    "            results.append({**item, **cedula, **details})\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "    return results\n",
    "\n",
    "def update_expediente(token, expediente_id, update):\n",
    "    res = get_cedula_by_expediente_id(token, expediente_id)\n",
    "    cedula_id = res[\"cedula\"][\"_id\"]\n",
    "    url = f\"https://api.olivia-fairlac.org/api/cedula/{cedula_id}?expediente={expediente_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    data = update\n",
    "    response = requests.patch(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"PATCH request was successful.\")\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "\n",
    "\n",
    "import whisperx\n",
    "import gc\n",
    "device = \"cuda\" \n",
    "batch_size = 8 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "# tmodel = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, language=\"es\")\n",
    "# model_a, metadata = whisperx.load_align_model(language_code=\"es\", device=device)\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "def download_audio(url, exp_id):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"pending/{exp_id}.wav\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(\"File downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c6cd6d22-7357-4a8f-ad0b-03dd0ed00e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = connect()\n",
    "# exps = get_expedientes(token)\n",
    "# for exp in exps:\n",
    "#     if len(exp['audios']) > 0 and exp[\"expediente\"][\"audio_procesado\"] == False:\n",
    "#         print(exp['expediente']['_id'])\n",
    "#         download_audio(exp['audios'][0]['url'], exp['expediente']['_id'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef499bdb-6d38-47bf-9071-3a1f9c606131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exps = get_expedientes(token)\n",
    "# for exp in exps:\n",
    "#     if len(exp['audios']) > 0 and exp[\"expediente\"][\"audio_procesado\"] == False:\n",
    "#         print(get_cedula_by_expediente_id(token, exp[\"expediente\"][\"_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad38ce68-7640-4073-96a2-48ecbcfd29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu117. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "device = \"cuda\" \n",
    "batch_size = 8 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "tmodel = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, language=\"es\")\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"es\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3a5ad4-ae37-4c71-9dd4-3af9adb8f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    command = f\"whisperx {audio_path} --model large-v2 --align_model WAV2VEC2_ASR_LARGE_LV60K_960H --batch_size 4 --compute_type int8 --output_dir transcriptions --language es --output_format vtt --hf_token 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\"\n",
    "    \n",
    "    # Split the command into a list of arguments\n",
    "    args = command.split()\n",
    "    \n",
    "    # Use subprocess to run the command\n",
    "    try:\n",
    "        subprocess.run(args, check=True)\n",
    "        return 0\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Command execution failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e5010e-062a-4ff0-a83c-17084fa6ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def transcribe_pending():\n",
    "    audios = glob(\"pending/*\")\n",
    "    for audio in audios:\n",
    "        print(f\"Processing {audio}\\n\")\n",
    "        start_time = time.time()\n",
    "        audiopath = Path(audio)\n",
    "        audio_id = audiopath.stem\n",
    "        r = transcribe(audiopath)\n",
    "        if r == 0:\n",
    "            try:\n",
    "                os.remove(audiopath)\n",
    "                print(\"Processed\")\n",
    "                print()\n",
    "            except OSError as e:\n",
    "                print(f\"Error removing the file: {e}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64723b4f-e044-47e3-bcd7-4fe85d4fba3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcribe_pending()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9fde9e6-8303-4e16-a3d4-85e3f6fdefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def answer_pending():\n",
    "    trans = glob(\"transcriptions/*\")\n",
    "    for audio in trans:\n",
    "        print(f\"Processing {audio}\\n\")\n",
    "        start_time = time.time()\n",
    "        audiopath = Path(audio)\n",
    "        audio_id = audiopath.stem\n",
    "        try:\n",
    "            update = get_update(audio_id)\n",
    "            update_expediente(token, audio_id, update)\n",
    "            try:\n",
    "                os.remove(audiopath)\n",
    "                print(\"Processed\")\n",
    "                print()\n",
    "            except OSError as e:\n",
    "                print(f\"Error removing the file: {e}\")\n",
    "                print()\n",
    "        except Exception as error:\n",
    "            print(f\"Error when processing {audio_id}:\", error)\n",
    "            print()\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(\"\\nExecution time:\", execution_time, \"seconds\")\n",
    "        print()\n",
    "        break\n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f551c10-1ad8-4a09-934f-89f8c88c942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcriptions/64dbb5286fbd221ffb8209ec.vtt\n",
      "\n",
      "Q: ¿Cuál es el nombre completo de la persona que está siendo entrevistada?\n",
      "A: El nombre completo de la persona que está siendo entrevistada es Juan Pérez.\n",
      "\n",
      "\n",
      "Q: ¿Con qué genero se identifica la persona que está siendo entrevistada?\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Cuál es el sexo de la persona que está siendo entrevistada?\n",
      "Available Categories ['mujer', 'hombre', 'intersexual']\n",
      "A: mujer\n",
      "\n",
      "\n",
      "Q: ¿Qué fecha de nacimiento tiene la persona que está siendo entrevistada?\n",
      "A: 25 de mayo de 1985.\n",
      "\n",
      "\n",
      "Q: ¿Qué nacionalidad tiene la persona que está siendo entrevistada?\n",
      "A: Ella es de España.\n",
      "\n",
      "\n",
      "Q: ¿En dónde nació la persona que está siendo entrevistada?\n",
      "A: El fragmento \"prueba neovión\" no proporciona información sobre el lugar de nacimiento de la persona being interviewed. La respuesta es NA.\n",
      "\n",
      "\n",
      "Q: ¿En dónde radica actualmente la persona que está siendo entrevistada?\n",
      "A: El entrevistado es un médico y su trabajo consiste en realizar pruebas de diagnóstico para pacientes con enfermedades graves. En el momento de la entrevista, se encuentra en el hospital donde trabaja, atendiendo a un paciente que ha sido diagnosticado con una enfermedad neurológica.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el número de contacto? Responde con el número telefónico.\n",
      "A: El número de contacto es 956-234-5678.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el domicilio de la persona que está siendo entrevistada?\n",
      "A: El domicilio de la persona que está siendo entrevistada es NA. No se puede determinar el domicilio de la persona basado en los fragmentos proporcionados.\n",
      "\n",
      "\n",
      "Q: ¿Qué escolaridad tiene la persona que está siendo entrevistada?\n",
      "Available Categories ['kinder_o_preescolar', 'primaria', 'secundaria', 'preparatoria_o_bachillerato', 'normal', 'carrera_tecnica_o_comercial', 'licenciatura_o_superior', 'posgrado', 'ninguno']\n",
      "A: secundaria\n",
      "\n",
      "\n",
      "Q: ¿La escolaridad de la persona que está siendo entrevistada está terminada?\n",
      "Available Categories ['en_curso', 'terminada', 'trunca']\n",
      "A: terminada\n",
      "\n",
      "\n",
      "Q: ¿Tiene seguridad social la persona que está siendo entrevistada?\n",
      "Available Categories ['imss', 'issste', 'pemex', 'sedena', 'insabi', 'privado']\n",
      "A: sedena\n",
      "\n",
      "\n",
      "Q: ¿A qué se dedica actualmente la persona entrevistada?\n",
      "Available Categories ['jornalera_o_albaniil', 'empleada_o_obrera_o', 'labores_del_hogar', 'estudios', 'negocio_propio', 'deporte', 'jubilado_pensionado', 'ninguna']\n",
      "A: empleada_o_obrera_o\n",
      "\n",
      "\n",
      "Q: ¿Qué estado civil tiene la persona que está siendo entrevistada?\n",
      "Available Categories ['union_libre', 'casada_o', 'separada_o', 'divorciada_o', 'viuda_o', 'soltera_o']\n",
      "A: divorciada_o\n",
      "\n",
      "\n",
      "Q: ¿Con qué régimen matrimonial está casada la persona que está siendo entrevistada?\n",
      "Available Categories ['separacion_de_bienes', 'sociedad_legal', 'sociedad_conyugal_o_voluntaria']\n",
      "A: sociedad_legal\n",
      "\n",
      "\n",
      "Q: ¿Cuáles son las características de la casa en la que vive la persona que está siendo entrevistada?\n",
      "Available Categories ['casa_independiente', 'departamento_en_edificio_o_unidad_habitacional', 'departamento_en_vecindad', 'cuarto_en_la_azotea', 'local_no_construido_para_habitacion', 'casa_o_departamento_en_terreno_familiar', 'casa_movil_refugio', 'asilo', 'orfanato_o_convento', 'no_tiene_vivienda']\n",
      "A: cuarto_en_la_azotea\n",
      "\n",
      "\n",
      "Q: ¿La vivienda en la que la persona que está siendo entrevistada es compartida?\n",
      "Available Categories ['amistades', 'familiares']\n",
      "A: familiares\n",
      "\n",
      "\n",
      "Q: ¿Cuántas personas viven en la casa de la persona que está siendo entrevistada?\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Quién aporta el mayor ingreso dentro del hogar?\n",
      "A: El marido.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el motivo de la atención (sé especifico)?\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Ha tenido que ser atendida en una institución médica o por personal médico como consecuencia de un evento de violencia con la persona agresora? Responde sí o no.\n",
      "A: False\n",
      "\n",
      "\n",
      "Q: ¿Cuál fue el último episodio de violencia? Describelo con detalle\n",
      "A: NA\n",
      "\n",
      "\n",
      "Q: Nombre de la persona agresora\n",
      "A: \n",
      "\n",
      "\n",
      "Q: Edad de la persona agresora\n",
      "A: 35 años\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el domicilio de la persona agresora? Responde calle y colonia.\n",
      "A: NA\n",
      "\n",
      "\n",
      "Q: ¿Cuál es la escolaridad de la persona agresora? Responde con el grado de estudios\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Cuál es la ocupación de la persona agresora?\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Cuál es el teléfono de la persona agresora?\n",
      "A: El teléfono de la persona agresora es NA. No se menciona en los fragmentos proporcionados.\n",
      "\n",
      "\n",
      "Q: ¿La persona agresora posee armas?\n",
      "A: False\n",
      "\n",
      "\n",
      "Q: ¿La persona agresora tiene vinculos con el crimen organizado?\n",
      "A: False\n",
      "\n",
      "\n",
      "Q: ¿Qué antecedentes penales tiene la persona agresora?\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Cuáles son las características físicas que tiene la persona agresora?\n",
      "A: \n",
      "\n",
      "\n",
      "PATCH request was successful.\n",
      "Processed\n",
      "\n",
      "\n",
      "Execution time: 157.71603274345398 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upt = answer_pending()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "70aff9e3-5777-4c6f-8c88-ef61fa64678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'II_nombre': 'El nombre completo de la persona que está siendo entrevistada es Juan Pérez.',\n",
       " 'II_genero_especificar': '',\n",
       " 'II_sexo': 'El sexo de la persona que está siendo entrevistada es NA. No se puede determinar el sexo a partir de los fragmentos proporcionados.',\n",
       " 'II_fecha_de_nacimiento': '25 de mayo de 1985.',\n",
       " 'II_nacionalidad': 'Ella es de España.',\n",
       " 'II_lugar_de_nacimiento': 'El fragmento \"prueba neovión\" no proporciona información sobre el lugar de nacimiento de la persona being interviewed. La respuesta es NA.',\n",
       " 'II_entidad_federativa_donde_reside_actualmente': 'El entrevistado es un médico y su trabajo consiste en realizar pruebas de diagnóstico para pacientes con enfermedades graves. En el momento de la entrevista, se encuentra en el hospital donde trabaja, atendiendo a un paciente que ha sido diagnosticado con una enfermedad neurológica.',\n",
       " 'II_telefono_fijo_casa': 'El número de contacto es 956-234-5678.',\n",
       " 'II_direccion_calle': 'El domicilio de la persona que está siendo entrevistada es NA. No se puede determinar el domicilio de la persona basado en los fragmentos proporcionados.',\n",
       " 'III_escolaridad': '',\n",
       " 'III_su_escolaridad_esta_en': '',\n",
       " 'III_cual_seguridad_social': '',\n",
       " 'III_ocupacion_de_la_persona': '',\n",
       " 'III_situacion_conyugal': '',\n",
       " 'III_regimen_matrimonial': 'El régimen matrimonial de la persona que está siendo entrevistada es NA.',\n",
       " 'III_tipo_de_vivienda': 'La casa es muy amplia y tiene un jardín en el frente.',\n",
       " 'III_compartida_con_otras_personas': '',\n",
       " 'III_cuantas_personas_habitan_en_su_vivienda': '',\n",
       " 'III_quien_aporta_el_mayor_ingreso_dentro_del_hogar': 'El marido.',\n",
       " 'IV_contexto_causa_y_evolucion': '',\n",
       " 'IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora': False,\n",
       " 'IV_ultimo_episodio_de_violencia_especificar': 'NA',\n",
       " 'VI_nombre': '',\n",
       " 'VI_edad': '35 años',\n",
       " 'VI_calle': 'NA',\n",
       " 'VI_escolaridad': '',\n",
       " 'VI_especificar_ocupacion': '',\n",
       " 'VI_telefono_fijo_casa': 'El teléfono de la persona agresora es NA. No se menciona en los fragmentos proporcionados.',\n",
       " 'VI_posesion_de_armas': False,\n",
       " 'VI_pertenece_o_tiene_enlace_con_el_crimen_organizado': False,\n",
       " 'VI_historial_de_antecedentes_penales': '',\n",
       " 'VI_especificar_senias': ''}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14af5109-9dcd-41b0-a80f-c57c5a4ef84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del upt[\"III_su_escolaridad_esta_en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "721e9fae-ceb2-4bf8-92cd-defcb59be63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_expediente(token, expediente_id, update):\n",
    "    res = get_cedula_by_expediente_id(token, expediente_id)\n",
    "    cedula_id = res[\"cedula\"][\"_id\"]\n",
    "    url = f\"https://api.olivia-fairlac.org/api/cedula/{cedula_id}?expediente={expediente_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    data = update\n",
    "    response = requests.patch(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"PATCH request was successful.\")\n",
    "    else:\n",
    "        raise Exception(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "20110ef0-ef22-453c-b34c-dbcbb927e94a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "{\"IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora\":{\"name\":\"ValidatorError\",\"message\":\"`` is not a valid enum value for path `IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora`.\",\"properties\":{\"message\":\"`` is not a valid enum value for path `IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora`.\",\"type\":\"enum\",\"enumValues\":[\"true\",\"false\"],\"path\":\"IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora\",\"value\":\"\"},\"kind\":\"enum\",\"path\":\"IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora\",\"value\":\"\"},\"VI_posesion_de_armas\":{\"name\":\"ValidatorError\",\"message\":\"`` is not a valid enum value for path `VI_posesion_de_armas`.\",\"properties\":{\"message\":\"`` is not a valid enum value for path `VI_posesion_de_armas`.\",\"type\":\"enum\",\"enumValues\":[\"true\",\"false\"],\"path\":\"VI_posesion_de_armas\",\"value\":\"\"},\"kind\":\"enum\",\"path\":\"VI_posesion_de_armas\",\"value\":\"\"},\"VI_pertenece_o_tiene_enlace_con_el_crimen_organizado\":{\"name\":\"ValidatorError\",\"message\":\"`` is not a valid enum value for path `VI_pertenece_o_tiene_enlace_con_el_crimen_organizado`.\",\"properties\":{\"message\":\"`` is not a valid enum value for path `VI_pertenece_o_tiene_enlace_con_el_crimen_organizado`.\",\"type\":\"enum\",\"enumValues\":[\"true\",\"false\"],\"path\":\"VI_pertenece_o_tiene_enlace_con_el_crimen_organizado\",\"value\":\"\"},\"kind\":\"enum\",\"path\":\"VI_pertenece_o_tiene_enlace_con_el_crimen_organizado\",\"value\":\"\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mupdate_expediente\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m64dbb5286fbd221ffb8209ec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 15\u001b[0m, in \u001b[0;36mupdate_expediente\u001b[0;34m(token, expediente_id, update)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATCH request was successful.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mException\u001b[0m: {\"IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora\":{\"name\":\"ValidatorError\",\"message\":\"`` is not a valid enum value for path `IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora`.\",\"properties\":{\"message\":\"`` is not a valid enum value for path `IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora`.\",\"type\":\"enum\",\"enumValues\":[\"true\",\"false\"],\"path\":\"IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora\",\"value\":\"\"},\"kind\":\"enum\",\"path\":\"IV_ha_tenido_que_ser_atendida_en_una_institucion_medica_o_por_personal_medico_como_consecuencia_de_un_evento_de_violencia_con_la_persona_agresora\",\"value\":\"\"},\"VI_posesion_de_armas\":{\"name\":\"ValidatorError\",\"message\":\"`` is not a valid enum value for path `VI_posesion_de_armas`.\",\"properties\":{\"message\":\"`` is not a valid enum value for path `VI_posesion_de_armas`.\",\"type\":\"enum\",\"enumValues\":[\"true\",\"false\"],\"path\":\"VI_posesion_de_armas\",\"value\":\"\"},\"kind\":\"enum\",\"path\":\"VI_posesion_de_armas\",\"value\":\"\"},\"VI_pertenece_o_tiene_enlace_con_el_crimen_organizado\":{\"name\":\"ValidatorError\",\"message\":\"`` is not a valid enum value for path `VI_pertenece_o_tiene_enlace_con_el_crimen_organizado`.\",\"properties\":{\"message\":\"`` is not a valid enum value for path `VI_pertenece_o_tiene_enlace_con_el_crimen_organizado`.\",\"type\":\"enum\",\"enumValues\":[\"true\",\"false\"],\"path\":\"VI_pertenece_o_tiene_enlace_con_el_crimen_organizado\",\"value\":\"\"},\"kind\":\"enum\",\"path\":\"VI_pertenece_o_tiene_enlace_con_el_crimen_organizado\",\"value\":\"\"}}"
     ]
    }
   ],
   "source": [
    "update_expediente(token, \"64dbb5286fbd221ffb8209ec\", upt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb19484-289d-47b3-bb6c-7c41ae913ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
