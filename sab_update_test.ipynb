{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716d937e-5854-4900-8a9a-a01c08526aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:23<00:00, 47.71s/it]\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# # Load model\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"  #'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = \"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\"\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id, use_auth_token=hf_auth, cache_dir=\"./hub\"\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_auth,\n",
    "    cache_dir=\"./hub\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# # Load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id, use_auth_token=hf_auth, cache_dir=\"./hub\"\n",
    ")\n",
    "\n",
    "stop_list = [\"\\nHuman:\", \"\\n```\\n\"]\n",
    "\n",
    "stop_token_ids = [tokenizer(x)[\"input_ids\"] for x in stop_list]\n",
    "\n",
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
    "    ) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids) :], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task=\"text-generation\",\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    ")\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import re\n",
    "from olivia_questions import questions\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686a4de-3110-4d83-9ac9-1d9f6cafa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Basado en los siguientes fragmentos de una entrevista, responde la pregunta del final (si en los fragmentos no se encuentra la respuesta a la pregunta del final, responde 'NA').\n",
    "\n",
    "Fragmentos:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c81f05b4-4086-4807-aebe-8f5485fcc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update(audio_id):\n",
    "    # load document\n",
    "    filepath = f\"./diari/{audio_id}.vtt\"\n",
    "    loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # load embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "    )\n",
    "\n",
    "    # clean documents\n",
    "    pattern = r\"\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}\\.\\d{3}\"\n",
    "    source_documents = []\n",
    "    for doc in documents:\n",
    "        doc.page_content = doc.page_content.replace(\"WEBVTT\", \"\")\n",
    "        doc.page_content = re.sub(pattern, \"\", doc.page_content)\n",
    "        doc.page_content = doc.page_content.replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "        source_documents.append(doc)\n",
    "\n",
    "    # split documents into texts\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
    "    texts = text_splitter.split_documents(source_documents)\n",
    "\n",
    "    # generate vectore store\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    # generate QA chain\n",
    "    prompt = PromptTemplate(\n",
    "        template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        # retriever=vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6}),\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        # verbose=True\n",
    "    )\n",
    "\n",
    "    # generate patch update\n",
    "    update = {}\n",
    "    for qn in questions:\n",
    "        query = qn[\"question\"]\n",
    "        print(\"Q:\", query)\n",
    "        answer = qa_chain.run(query)\n",
    "        answer = answer.split(\"\\n\")[0].strip()\n",
    "        if \"categories\" in qn.keys():\n",
    "            print(\"Available Categories\", qn[\"categories\"])\n",
    "            cats = Chroma.from_texts(qn[\"categories\"], embeddings)\n",
    "            scores = cats.similarity_search_with_score(answer)\n",
    "            highest = max(scores, key=lambda x: x[1])\n",
    "            if highest[1] < 0.5 and qn[\"otherKey\"] is not None:\n",
    "                update[qn[\"otherKey\"]] = answer\n",
    "                print(\"A:\", answer)\n",
    "            else:\n",
    "                update[qn[\"key\"]] = highest[0]\n",
    "                print(\"A:\", highest[0].page_content)\n",
    "        else:\n",
    "            update[qn[\"key\"]] = answer\n",
    "            print(\"A:\", answer)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return update\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def connect():\n",
    "    # change endpoint and data\n",
    "    # url = f\"http://localhost:8080/api/login\"\n",
    "    url = f\"https://api.olivia-fairlac.org/api/login\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    data = {\"email\": \"david@gmail.com\", \"password\": \"ol1v14\"}\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        res = response.json()\n",
    "        token = res[\"token\"]\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_expedientes(token):\n",
    "    # TODO: change endpoint\n",
    "    url = f\"https://api.olivia-fairlac.org/api/expediente\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        audio_list = response.json()\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "    return audio_list\n",
    "\n",
    "def update_expediente(token, audio_id, update):\n",
    "    # TODO: change endpoint\n",
    "    url = f\"https://api.example.com/resource/{audio_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    data = update\n",
    "    response = requests.patch(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"PATCH request was successful.\")\n",
    "    else:\n",
    "        raise Exception(response.text)\n",
    "\n",
    "\n",
    "import whisperx\n",
    "import gc\n",
    "device = \"cuda\" \n",
    "batch_size = 8 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "# tmodel = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, language=\"es\")\n",
    "# model_a, metadata = whisperx.load_align_model(language_code=\"es\", device=device)\n",
    "\n",
    "def transcribe(audiopath):\n",
    "    # 1. Transcribe with original whisper (batched)\n",
    "    audio = whisperx.load_audio(audiopath)\n",
    "    result = tmodel.transcribe(audio, batch_size=batch_size)\n",
    "    # 2. Align whisper output\n",
    "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "    prediction = \" \".join([s[\"text\"] for s in result[\"segments\"]])\n",
    "    return prediction\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def run():\n",
    "    update = get_update(\"F-285\")\n",
    "    # token = connect()\n",
    "    # audios = glob(\"audio_samples\")\n",
    "    # print(token)\n",
    "    # for audio in audios:\n",
    "    #     print(f\"Processing {audio_id}\\n\")\n",
    "    #     start_time = time.time()\n",
    "    #     audiopath = Path(audio)\n",
    "    #     audio_id = audiopath.stem\n",
    "    #     transcription = transcribe(audiopath)\n",
    "    #     try:\n",
    "    #         update = get_update(audio_id)\n",
    "    #         update_expediente(token, audio_id, update)\n",
    "    #     except Exception as error:\n",
    "    #         print(f\"Error when processing {audio_id}:\", error)\n",
    "    #     end_time = time.time()\n",
    "    #     execution_time = end_time - start_time\n",
    "    #     print(\"\\nExecution time:\", execution_time, \"seconds\")\n",
    "    #     print()\n",
    "\n",
    "\n",
    "def download_audio(exp_id):\n",
    "    url = f\"https://olivia-files.s3.us-east-1.amazonaws.com/{exp_id}/audios/audio_{exp_id}.wav?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAXNHFTEMUZXBPW7NQ%2F20230815%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230815T175450Z&X-Amz-Expires=86400&X-Amz-Signature=574f38123cd5d7a0370f56adeec1eac099bba2897458ca47ac23af1103f2255e&X-Amz-SignedHeaders=host&x-id=GetObject\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{exp_id}.wav\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(\"File downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6cd6d22-7357-4a8f-ad0b-03dd0ed00e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64dbb5286fbd221ffb8209ec\n",
      "File downloaded successfully.\n",
      "\n",
      "64d40e69dcd910569f2ac28a\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d3ed62dcd910569f2ac188\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d2cd10386122773adc6a19\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d27a79386122773adc6967\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d27665386122773adc68cc\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d2666f386122773adc6826\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d12a86849d7160c7314aed\n",
      "Failed to download file. Status code: 403\n",
      "\n",
      "64d1150d849d7160c7314ac2\n",
      "Failed to download file. Status code: 403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = connect()\n",
    "exps = get_expedientes(token)[\"docs\"]\n",
    "for exp in exps:\n",
    "    if exp[\"audio_procesado\"] == False:\n",
    "        print(exp['_id'])\n",
    "        download_audio(exp['_id'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f551c10-1ad8-4a09-934f-89f8c88c942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Cuál es el nombre completo de la persona que está siendo entrevistada?\n",
      "A: No se puede determinar con certeza el nombre completo de la persona que está siendo entrevistada basado en los fragmentos proporcionados.\n",
      "\n",
      "\n",
      "Q: ¿Con qué genero se identifica la persona que está siendo entrevistada? Responde 'femenino' o 'masculino'\n",
      "A: Femenino\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el sexo de la persona que está siendo entrevistada? Responde 'hombre' o 'mujer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Categories ['mujer', 'hombre', 'intersexual']\n",
      "A: hombre\n",
      "\n",
      "\n",
      "Q: ¿Qué fecha de nacimiento tiene la persona que está siendo entrevistada?\n",
      "A: 19 de septiembre del 82.\n",
      "\n",
      "\n",
      "Q: ¿Qué nacionalidad tiene la persona que está siendo entrevistada?\n",
      "A: Mexicana\n",
      "\n",
      "\n",
      "Q: ¿En dónde nació la persona que está siendo entrevistada?\n",
      "A: No se especifica.\n",
      "\n",
      "\n",
      "Q: ¿En dónde radica actualmente la persona que está siendo entrevistada?\n",
      "A: No sé, mi pareja es extranjero.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el número de contacto? Responde con el número telefónico.\n",
      "A: El número de contacto es 3319-71-4459.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el domicilio de la persona que está siendo entrevistada?\n",
      "A: Campo Castillo número 1139.\n",
      "\n",
      "\n",
      "Q: ¿Qué escolaridad tiene la persona que está siendo entrevistada?\n",
      "Available Categories ['kinder_o_preescolar', 'primaria', 'secundaria', 'preparatoria_o_bachillerato', 'normal', 'carrera_tecnica_o_comercial', 'licenciatura_o_superior', 'posgrado', 'ninguno']\n",
      "A: licenciatura_o_superior\n",
      "\n",
      "\n",
      "Q: ¿La escolaridad de la persona que está siendo entrevistada está terminada?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Categories ['en_curso', 'terminada', 'trunca']\n",
      "A: en_curso\n",
      "\n",
      "\n",
      "Q: ¿Tiene seguridad social la persona que está siendo entrevistada?\n",
      "Available Categories ['imss', 'issste', 'pemex', 'sedena', 'insabi', 'privado']\n",
      "A: privado\n",
      "\n",
      "\n",
      "Q: ¿A qué se dedica actualmente la persona entrevistada?\n",
      "Available Categories ['jornalera_o_albaniil', 'empleada_o_obrera_o', 'labores_del_hogar', 'estudios', 'negocio_propio', 'deporte', 'jubilado_pensionado', 'ninguna']\n",
      "A: empleada_o_obrera_o\n",
      "\n",
      "\n",
      "Q: ¿Qué estado civil tiene la persona que está siendo entrevistada?\n",
      "Available Categories ['union_libre', 'casada_o', 'separada_o', 'divorciada_o', 'viuda_o', 'soltera_o']\n",
      "A: separada_o\n",
      "\n",
      "\n",
      "Q: ¿Con qué régimen matrimonial está casada la persona que está siendo entrevistada?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Categories ['separacion_de_bienes', 'sociedad_legal', 'sociedad_conyugal_o_voluntaria']\n",
      "A: sociedad_legal\n",
      "\n",
      "\n",
      "Q: ¿Cuáles son las características de la casa en la que vive la persona que está siendo entrevistada?\n",
      "Available Categories ['casa_independiente', 'departamento_en_edificio_o_unidad_habitacional', 'departamento_en_vecindad', 'cuarto_en_la_azotea', 'local_no_construido_para_habitacion', 'casa_o_departamento_en_terreno_familiar', 'casa_movil_refugio', 'asilo', 'orfanato_o_convento', 'no_tiene_vivienda']\n",
      "A: local_no_construido_para_habitacion\n",
      "\n",
      "\n",
      "Q: ¿La vivienda en la que la persona que está siendo entrevistada es compartida?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Categories ['amistades', 'familiares']\n",
      "A: familiares\n",
      "\n",
      "\n",
      "Q: ¿Cuántas personas viven en la casa de la persona que está siendo entrevistada?\n",
      "A: Tres.\n",
      "\n",
      "\n",
      "Q: ¿Quién aporta el mayor ingreso dentro del hogar?\n",
      "A: No sé.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el motivo de la atención (sé especifico)?\n",
      "A: No hay un motivo específico de atención, sino que se trata de una entrevista para recopilar información sobre una situación de defraudación y violencia en el ámbito familiar.\n",
      "\n",
      "\n",
      "Q: ¿Ha tenido que ser atendida en una institución médica o por personal médico como consecuencia de un evento de violencia con la persona agresora? Responde sí o no.\n",
      "A: No\n",
      "\n",
      "\n",
      "Q: ¿Cuál fue el último episodio de violencia? Describelo con detalle\n",
      "A: No, no ha habido violencia reciente.\n",
      "\n",
      "\n",
      "Q: Nombre de la persona agresora\n",
      "A: No se encuentra la respuesta en los fragmentos proporcionados.\n",
      "\n",
      "\n",
      "Q: Edad de la persona agresora\n",
      "A: No se menciona la edad de la persona agresora en los fragmentos proporcionados.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el domicilio de la persona agresora? Responde calle y colonia.\n",
      "A: No tengo información.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es la escolaridad de la persona agresora? Responde con el grado de estudios\n",
      "A: No se menciona la escolaridad de la persona agresora en los fragmentos proporcionados.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es la ocupación de la persona agresora?\n",
      "A: No se menciona la ocupación de la persona agresora en los fragmentos proporcionados. La respuesta a la pregunta del final es 'NA'.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el teléfono de la persona agresora?\n",
      "A: No se encuentra la respuesta en los fragmentos.\n",
      "\n",
      "\n",
      "Q: ¿La persona agresora posee armas?\n",
      "A: No.\n",
      "\n",
      "\n",
      "Q: ¿La persona agresora tiene vinculos con el crimen organizado?\n",
      "A: NA\n",
      "\n",
      "\n",
      "Q: ¿Qué antecedentes penales tiene la persona agresora?\n",
      "A: NA\n",
      "\n",
      "\n",
      "Q: ¿Cuáles son las características físicas que tiene la persona agresora?\n",
      "A: No puedo decir, no sé.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aff9e3-5777-4c6f-8c88-ef61fa64678b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
