{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2702d79-c779-4e90-910c-6d5adb770472",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe55e6f-0c8d-4d09-996d-00758500412c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:13<00:00, 64.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf' #'meta-llama/Llama-2-70b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth,\n",
    "    cache_dir=\"./hub\"\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth,\n",
    "    cache_dir=\"./hub\"\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f7c08-3ea0-4e55-91bd-ded12c632990",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e4f60e-3324-45f8-93b9-390098d9efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth,\n",
    "    cache_dir=\"./hub\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c1340-1d4c-438c-a1ab-79f99c020b00",
   "metadata": {},
   "source": [
    "Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc7acaf5-dcf1-4294-a4d9-0c2d625948f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n",
    "\n",
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc43ced1-f5a7-4852-bc9d-5d71f9d987a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25277c-0457-419c-a8f6-06fdfab45613",
   "metadata": {},
   "source": [
    "Implement LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f2f372-af14-4804-b11f-27f9168b4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219e4eb-705a-46ad-b3ce-e185dccedfbb",
   "metadata": {},
   "source": [
    "# Test 1: Subtítulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06300117-aa22-4485-8a4a-9598eac50aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "filepath = \"./diari/F-490.vtt\"\n",
    "loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf027d-5fdc-4441-a8f1-901b12f60134",
   "metadata": {},
   "source": [
    "Borrar identificador de speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a53735-a1d7-43cc-ab11-2b8de48448a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    doc.page_content = doc.page_content.replace(\"WEBVTT\", \"\")\n",
    "    # print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5638a211-f271-46ac-8a79-24e9f2f1eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n00:00.448 --> 00:01.850\\nMuchas gracias, Pedro.\\n\\n00:01.850 --> 00:04.092\\nEntonces vamos a iniciar l'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0474a6f-fe41-49b8-8a09-01a0977270f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)c7362/.gitattributes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 439kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 237kB/s]\n",
      "Downloading (…)de792c7362/README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 4.51k/4.51k [00:00<00:00, 4.16MB/s]\n",
      "Downloading (…)792c7362/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 701/701 [00:00<00:00, 851kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 123/123 [00:00<00:00, 154kB/s]\n",
      "Downloading (…)c7362/eval/readme.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4.00/4.00 [00:00<00:00, 5.02kB/s]\n",
      "Downloading (…)_sts-dev_results.csv: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 770/770 [00:00<00:00, 1.04MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 439M/439M [00:02<00:00, 181MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 16.8kB/s]\n",
      "Downloading (…)-mt-test_results.csv: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 299/299 [00:00<00:00, 400kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 126kB/s]\n",
      "Downloading (…)c7362/tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 480k/480k [00:00<00:00, 36.8MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 556/556 [00:00<00:00, 735kB/s]\n",
      "Downloading (…)de792c7362/vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 242k/242k [00:00<00:00, 5.68MB/s]\n",
      "Downloading (…)92c7362/modules.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 229/229 [00:00<00:00, 300kB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='hiiamsid/sentence_similarity_spanish_es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e8a270f-a65b-4963-9533-aca16d771a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7118ead-72ea-43ff-9205-2206e3e282f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHOSE = \"la persona que está siendo entrevistada\"\n",
    "\n",
    "questions = [\n",
    "    f\"¿Cuál es el nombre completo de {WHOSE}?\",\n",
    "    f\"¿Con qué genero se identifica {WHOSE}?\",\n",
    "    f\"¿Cuál es el sexo de {WHOSE}?\",\n",
    "    f\"¿Cuál es la fecha de nacimiento de {WHOSE}?\",\n",
    "    f\"¿Qué nacionalidad tiene {WHOSE}?\",\n",
    "    f\"¿En qué lugar nació {WHOSE}?\",\n",
    "    f\"¿En qué entidad reside {WHOSE}?\",\n",
    "    f\"¿En dónde nació {WHOSE}?\",\n",
    "    f\"¿Cuál es su teléfono de contacto de {WHOSE}?\",\n",
    "    f\"¿Cuál es el domicilio de {WHOSE}?\",\n",
    "    f\"¿Qué escolaridad tiene {WHOSE}?\",\n",
    "    f\"¿La escolaridad de {WHOSE} está terminada?\",\n",
    "    f\"¿Tiene seguridad social {WHOSE}?\",\n",
    "    f\"¿A qué se dedica {WHOSE} actualmente?\",\n",
    "    f\"¿Qué estado civil tiene {WHOSE}?\",\n",
    "    f\"¿Con qué régimen matrimonial está casada {WHOSE}?\",\n",
    "    f\"¿Cuáles son las características de la casa en la que vive {WHOSE}?\",\n",
    "    f\"¿La vivienda en la que {WHOSE} es compartida?\",\n",
    "    f\"¿Cuántas personas viven en la casa de {WHOSE}?\",\n",
    "    f\"¿Cuáles son los datos de las personas con las que vive {WHOSE}?\",\n",
    "    f\"¿Quién aporta el mayor ingreso dentro del hogar de {WHOSE}?\",\n",
    "    f\"¿Cuál es el motivo de la atención para {WHOSE}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0d04084-375e-4418-9e7a-68c6b6837f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f06743a7-c6b0-4b7a-bedf-5b05577d796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit qa.run(\"¿Cual es la fecha de nacimiento de la persona que está siendo entrevistada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa44b258-e977-4d2d-91de-6a50d5c2823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query in questions:\n",
    "#     answer = qa.run(query)\n",
    "#     answer = answer.split(\"\\n\")[0].strip()\n",
    "#     print(\"Q:\", query)\n",
    "#     print(\"A:\", answer)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c38c4-1263-4698-a326-496050af9f41",
   "metadata": {},
   "source": [
    "#### Usando custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7374300c-2d54-4f7c-b9f9-cb266f9d2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_INST, E_INST = \"\", \"\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "instruction = B_INST + \"\"\"Use ONLY the following pieces of context (coming from an interview) to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\" + E_INST\n",
    "\n",
    "prompt_template = instruction + \"\"\"\n",
    "Answer in Spanish:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "607217c4-17a3-40e6-a119-a15fbd82a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 5}),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    # verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b270a85-7aa4-41dd-9b46-178192425818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query in questions:\n",
    "#     answer = qa.run(query)\n",
    "#     answer = answer.split(\"\\n\")[0].strip()\n",
    "#     print(\"Q:\", query)\n",
    "#     print(\"A:\", answer)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d077d7-4ec3-46d7-bce7-acbfb2bc0169",
   "metadata": {},
   "source": [
    "# Diálogos con contexto de dos líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c49d075-76f7-403b-a373-c8608a461d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='hiiamsid/sentence_similarity_spanish_es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a60eaca6-9c40-4d20-8e7a-7e3eb5ef4d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00:00.448 --> 00:01.850\\nMuchas gracias, Pedro.',\n",
       " '00:01.850 --> 00:04.092\\nEntonces vamos a iniciar la entrevista.',\n",
       " '00:04.092 --> 00:07.195\\nEs importante que tú sepas que estos servicios no se te van a cobrar.',\n",
       " '00:07.195 --> 00:22.308\\nLa información que tú me compartas el día de hoy se queda de manera confidencial aquí con nosotros y nosotros somos una unidad que te va a brindar orientación, información con el trámite que tú deseas realizar, que me compartes que es divorcio, ¿de acuerdo?',\n",
       " '00:22.308 --> 00:24.531\\nDe aquí te derivaremos a otra institución.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./diari/F-490-dialogues.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "lines = [part.split(\"]: \")[-1] for num, part in enumerate(text.split(\"\\n\\n\"))]\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3c9fc5e-1485-4aa5-9206-76ec6b6985f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = []\n",
    "i = 0\n",
    "prev = None\n",
    "prevText = \"\"\n",
    "curr = \"\"\n",
    "while i < len(lines):\n",
    "    curr_type = \"question\" if \"¿\" in lines[i] or \"?\" in lines[i] else \"text\"\n",
    "    # if current line has a question\n",
    "    if curr_type == \"question\":\n",
    "        # if the prev line had only text\n",
    "        if prev == \"text\":\n",
    "            dialogues.append(curr)\n",
    "            curr = \"\"\n",
    "    # add current text to curr\n",
    "    link = \"\\n\\n\" if curr != \"\" else \"\"\n",
    "    curr = curr + link + lines[i]\n",
    "\n",
    "    # set prev\n",
    "    prev = curr_type\n",
    "    \n",
    "    # increment i\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8365e9b-5dc9-4082-9de4-a3ee274ebfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00.448 --> 00:01.850\n",
      "Muchas gracias, Pedro.\n",
      "\n",
      "00:01.850 --> 00:04.092\n",
      "Entonces vamos a iniciar la entrevista.\n",
      "\n",
      "00:04.092 --> 00:07.195\n",
      "Es importante que tú sepas que estos servicios no se te van a cobrar.\n",
      "-----------\n",
      "00:07.195 --> 00:22.308\n",
      "La información que tú me compartas el día de hoy se queda de manera confidencial aquí con nosotros y nosotros somos una unidad que te va a brindar orientación, información con el trámite que tú deseas realizar, que me compartes que es divorcio, ¿de acuerdo?\n",
      "\n",
      "00:22.308 --> 00:24.531\n",
      "De aquí te derivaremos a otra institución.\n",
      "\n",
      "00:26.275 --> 00:31.957\n",
      "Muy bien Perla, yo copié algunos datos de tu registro, igual voy a corroborar dicha información.\n",
      "-----------\n",
      "00:31.957 --> 00:35.698\n",
      "¿Tu nombre completo Perla Elizabeth Gómez Torres?\n",
      "\n",
      "00:35.698 --> 00:36.139\n",
      "Sí.\n",
      "-----------\n",
      "00:36.139 --> 00:39.620\n",
      "¿Te identificas como género femenino, sexo mujer?\n",
      "\n",
      "00:39.620 --> 00:42.141\n",
      "Sí.\n",
      "-----------\n",
      "00:42.141 --> 00:44.021\n",
      "¿Tu fecha de nacimiento cuál es?\n",
      "\n",
      "00:44.021 --> 00:48.483\n",
      "El 11 de octubre del 87.\n",
      "-----------\n",
      "00:48.483 --> 00:51.804\n",
      "¿Tienes 35 años cumplidos hasta el día de hoy?\n",
      "\n",
      "00:51.804 --> 00:51.844\n",
      "Sí.\n",
      "-----------\n",
      "00:51.844 --> 00:52.704\n",
      "¿Dónde naciste?\n",
      "\n",
      "00:52.704 --> 00:53.405\n",
      "¿En qué lugar?\n",
      "\n",
      "00:53.405 --> 00:54.105\n",
      "Aquí en Málaga.\n",
      "\n",
      "00:56.167 --> 00:58.627\n",
      "Entonces tu nacionalidad es mexicana.\n",
      "\n",
      "00:58.627 --> 00:58.787\n",
      "Sí.\n",
      "\n",
      "00:58.787 --> 01:02.068\n",
      "Muy bien.\n",
      "-----------\n",
      "01:02.068 --> 01:03.968\n",
      "Y radicas en Guadalajara, ¿verdad?\n",
      "\n",
      "01:03.968 --> 01:06.729\n",
      "Sí, es correcto.\n",
      "-----------\n",
      "01:06.729 --> 01:07.769\n",
      "¿Tu número de contacto 3319-7588-72?\n",
      "\n",
      "01:07.769 --> 01:07.869\n",
      "Sí.\n",
      "-----------\n",
      "01:07.869 --> 01:08.769\n",
      "¿Tu domicilio Tomás 2A41-82?\n",
      "\n",
      "01:08.769 --> 01:09.849\n",
      "¿Tienes algún número interior?\n",
      "\n",
      "01:09.849 --> 01:10.049\n",
      "No.\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for d in dialogues[:10]:\n",
    "    print(d)\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6cfe1e5-3ff4-4b39-903b-68e3b483daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(dialogues, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2740ea13-3186-40b4-ae70-b1b4ed39ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "handler = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "667b68a0-5e2a-41f5-9c5e-26012329b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "016d7f5a-3527-41ae-bbbe-2acd6a969aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' La fecha de nacimiento de la persona que está siendo entrevistada es el 11 de octubre del 87, esto se puede inferir de la conversación en la línea 00:44.021 --> 00:48.483.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"¿Cual es la fecha de nacimiento de la persona que está siendo entrevistada? Justifica tu respuesta\"\n",
    "# qa.run(query, callbacks=[handler])\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44448bb-3d46-4387-9e0a-afb2e3722eeb",
   "metadata": {},
   "source": [
    "#### Prueba con custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d7f26d6-db75-4288-8ac2-a02cc5beac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8565814e-fbed-4056-b4bb-eeb7d1c38d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Based on the following extracts of an interview, answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer in Spanish:\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Basado en los siguientes fragmentos de una entrevista, contesta la pregunta del final (si no encuentras la respuesta simplemente contesta 'NA').\n",
    "\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "684c4e4d-39ef-442f-a07d-ae201744fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "085d7174-36a3-4e1c-ad7e-49cc5682e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d28ed63b-a8a2-40a2-b7ef-a49fbf0318cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rels = docsearch.as_retriever().get_relevant_documents(\"escolaridad\")\n",
    "# ctx = \"\\n---------\\n\".join([rel.page_content for rel in rels])\n",
    "# print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "462c1f1f-0769-4ba4-8444-aebef0203453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = chain.run(question=\"¿Qué escolaridad tiene la persona que está siendo entrevistada\", context=ctx)\n",
    "# answer = answer.strip().split(\"\\n\")[0].strip()\n",
    "# answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "800c7f81-c081-4ca6-9837-e40a7dda621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query, k=4):\n",
    "    search_kwargs = {\"k\":k}\n",
    "    rels = docsearch.as_retriever(search_type=\"mmr\", search_kwargs=search_kwargs).get_relevant_documents(query=query)\n",
    "    ctx = \"\\n---------\\n\".join([rel.page_content for rel in rels])\n",
    "    return ctx\n",
    "\n",
    "def answer_question(question, context, verbose=False):\n",
    "    callbacks = [handler] if verbose else []\n",
    "    answer = chain.run(question=question, context=ctx, callbacks=callbacks)\n",
    "    answer = answer.strip().split(\"\\n\")[0].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6363bb3c-0ea5-47ea-b643-38a63a79e5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preparatoria'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = get_context(\"escolaridad\", k=5)\n",
    "answer_question(\"¿Qué escolaridad tiene la persona que está siendo entrevistada\", ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "047c3fa6-f2e1-4c44-b8e6-cac26678c659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prestada.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = get_context(\"residencia actual\", k=5)\n",
    "answer_question(\"¿En dónde reside actualmente la persona que está siendo entrevistada?\", ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62dca5a8-9417-4196-a9f3-27f61a70f6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El 11 de octubre del 87.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = get_context(\"tu fecha de nacimiento\")\n",
    "answer_question(\"¿Qué fecha de nacimiento tiene la persona que está siendo entrevistada?\", ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa74f558-a127-4f20-96da-bac01f22701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¿Tu fecha de nacimiento cuál es?\n",
      "\n",
      "\n",
      "El 11 de octubre del 87.\n",
      "-------\n",
      "\n",
      "\n",
      "¿En qué colonia?\n",
      "\n",
      "\n",
      "Monumental.\n",
      "-------\n",
      "\n",
      "\n",
      "Vallesa con Z?\n",
      "\n",
      "\n",
      "Con Z. Doble L y Z. Vallesa.\n",
      "-------\n",
      "\n",
      "\n",
      "¿A qué se dedica?\n",
      "\n",
      "\n",
      "Es empleada de domesticación.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "pattern = r'\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}\\.\\d{3}'\n",
    "\n",
    "tmpdialogues = [re.sub(pattern, \"\", dialogue) for dialogue in dialogues]\n",
    "tmpdocs = FAISS.from_texts(tmpdialogues, embedding=embeddings)\n",
    "rels = tmpdocs.as_retriever(search_type=\"mmr\").get_relevant_documents(query=\"fecha de nacimiento\")\n",
    "\n",
    "for rel in rels:\n",
    "    print(rel.page_content)\n",
    "    print(\"-------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e53bf1ff-74a0-4e73-b4c4-771af0f245d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_v2(query, k=4):\n",
    "    search_kwargs = {\"k\":k}\n",
    "    rels = tmpdocs.as_retriever(search_type=\"mmr\", search_kwargs=search_kwargs).get_relevant_documents(query=query)\n",
    "    ctx = \"\\n---------\\n\".join([rel.page_content for rel in rels])\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a42de759-01df-4316-9e65-c39625a2faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBasado en los siguientes fragmentos de una entrevista, contesta la pregunta del final (si no encuentras la respuesta simplemente contesta 'NA').\n",
      "\n",
      "\n",
      "¿Cuál es?\n",
      "\n",
      "\n",
      "Sí, es Monte Alegre.\n",
      "\n",
      "\n",
      "Sería el domicilio eventual.\n",
      "\n",
      "\n",
      "Monte Alegre.\n",
      "\n",
      "\n",
      "612.\n",
      "---------\n",
      "\n",
      "¿A qué se dedica?\n",
      "\n",
      "\n",
      "Es empleada de domesticación.\n",
      "---------\n",
      "\n",
      "¿Tu casa donde vives actualmente es prestada, rentada o propia?\n",
      "\n",
      "\n",
      "Prestada.\n",
      "---------\n",
      "\n",
      "¿Perteneces a algún grupo original, indígena?\n",
      "\n",
      "\n",
      "No.\n",
      "---------\n",
      "\n",
      "Una unidad... ¿Cómo se llama?\n",
      "\n",
      "\n",
      "De un hospital particular.\n",
      "\n",
      "\n",
      "Sobre esa calle.\n",
      "---------\n",
      "\n",
      "Él vive en el domicilio que me compartiste en el registro, ¿verdad?\n",
      "\n",
      "\n",
      "Sí.\n",
      "---------\n",
      "\n",
      "¿En qué colonia?\n",
      "\n",
      "\n",
      "Monumental.\n",
      "---------\n",
      "\n",
      "Y radicas en Guadalajara, ¿verdad?\n",
      "\n",
      "\n",
      "Sí, es correcto.\n",
      "---------\n",
      "\n",
      "¿A qué te dedicas actualmente?\n",
      "\n",
      "\n",
      "Trabajo en una empresa de telecomunicaciones.\n",
      "\n",
      "Pregunta: ¿En dónde radica actualmente la persona que está siendo entrevistada?\n",
      "Respuesta:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Monte Alegre.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = get_context_v2(\"residencia actual\", k=9)\n",
    "# ctx = \"\"\"\n",
    "# Y radicas en Guadalajara, ¿verdad?\n",
    "# Sí, es correcto.\n",
    "# \"\"\"\n",
    "answer_question(\"¿En dónde radica actualmente la persona que está siendo entrevistada?\", ctx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69923fc1-acf1-4e30-a3fb-8b50fa774e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El 11 de octubre del 87.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = get_context_v2(\"fecha de nacimiento\")\n",
    "answer_question(\"¿Qué fecha de nacimiento tiene la persona que está siendo entrevistada?\", ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2891b587-2ae0-4dd7-84e1-1813f6198fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Basado en los siguientes fragmentos de una entrevista, responde la pregunta del final (si en los fragmentos no se encuentra la respuesta a la pregunta del final, contesta 'NA').\n",
    "\n",
    "Fragmentos:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "\n",
    "pattern = r'\\d{2}:\\d{2}\\.\\d{3} --> \\d{2}:\\d{2}\\.\\d{3}'\n",
    "source_documents = []\n",
    "for doc in documents:\n",
    "    doc.page_content = doc.page_content.replace(\"WEBVTT\", \"\")\n",
    "    doc.page_content = re.sub(pattern, \"\", doc.page_content)\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "    source_documents.append(doc)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
    "texts = text_splitter.split_documents(source_documents)\n",
    "\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    # retriever=vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6}),\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 5}),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    # verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3304b528-2b37-4f0f-a626-a19e168df98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHOSE = \"la persona que está siendo entrevistada\"\n",
    "\n",
    "questions = [\n",
    "    f\"¿Cuál es el nombre completo de {WHOSE}?\",\n",
    "    f\"¿Con qué genero se identifica {WHOSE}? Responde 'femenino' o 'masculino'\",\n",
    "    f\"¿Cuál es el sexo de {WHOSE}? Responde 'hombre' o 'mujer'\",\n",
    "    f\"¿Qué fecha de nacimiento tiene {WHOSE}?\",\n",
    "    f\"¿Qué nacionalidad tiene {WHOSE}?\",\n",
    "    f\"¿En dónde nació {WHOSE}?\",\n",
    "    f\"¿En dónde radica actualmente {WHOSE}?\",\n",
    "    f\"¿Cuál es el número de contacto? Responde con el número telefónico.\",\n",
    "    f\"¿Cuál es el domicilio de {WHOSE}?\",\n",
    "    f\"¿Qué escolaridad tiene {WHOSE}?\",\n",
    "    f\"¿La escolaridad de {WHOSE} está terminada?\",\n",
    "    f\"¿Tiene seguridad social {WHOSE}?\",\n",
    "    f\"¿A qué se dedica actualmente la persona entrevistada?\",\n",
    "    f\"¿Qué estado civil tiene {WHOSE}?\",\n",
    "    f\"¿Con qué régimen matrimonial está casada {WHOSE}?\",\n",
    "    f\"¿Cuáles son las características de la casa en la que vive {WHOSE}?\",\n",
    "    f\"¿La vivienda en la que {WHOSE} es compartida?\",\n",
    "    f\"¿Cuántas personas viven en la casa de {WHOSE}?\",\n",
    "    f\"¿Cuáles son las características de las personas con las que vive?\",\n",
    "    f\"¿Quién aporta el mayor ingreso dentro del hogar?\",\n",
    "    f\"¿Cuál es el motivo de la atención (sé especifico)?\",\n",
    "    f\"¿Ha tenido que ser atendida en una institución médica o por personal médico como consecuencia de un evento de violencia con la persona agresora?\",\n",
    "    f\"¿Cuál fue el último episodio de violencia? Describelo con detalle\",\n",
    "    f\"Nombre de la persona agresora\",\n",
    "    f\"¿Cuál es el domicilio de Omar Alejandro? Responde calle y colonia.\",\n",
    "    f\"¿Cuál es la escolaridad de Omar Alejandro? Responde con el grado de estudios\",\n",
    "    f\"¿Omar Alejandro posee armas?\",\n",
    "    f\"¿Omar Alejandro tiene vinculos con el crimen organizado?\",\n",
    "    f\"¿Quiénes son tu red de apoyo?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6fc7d75f-ca43-467b-81b4-beec894df150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Cuál es el nombre completo de la persona que está siendo entrevistada?\n",
      "A: Perla Elizabeth Gómez Torres.\n",
      "\n",
      "\n",
      "Q: ¿Con qué genero se identifica la persona que está siendo entrevistada? Responde 'femenino' o 'masculino'\n",
      "A: Femenino\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el sexo de la persona que está siendo entrevistada? Responde 'hombre' o 'mujer'\n",
      "A: Mujer\n",
      "\n",
      "\n",
      "Q: ¿Qué fecha de nacimiento tiene la persona que está siendo entrevistada?\n",
      "A: 11 de octubre del 87.\n",
      "\n",
      "\n",
      "Q: ¿Qué nacionalidad tiene la persona que está siendo entrevistada?\n",
      "A: Sí, soy mexicana.\n",
      "\n",
      "\n",
      "Q: ¿En dónde nació la persona que está siendo entrevistada?\n",
      "A: Aquí en Málaga.\n",
      "\n",
      "\n",
      "Q: ¿En dónde radica actualmente la persona que está siendo entrevistada?\n",
      "A: En Guadalajara, México.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el número de contacto? Responde con el número telefónico.\n",
      "A: 3319-7588-72.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el domicilio de la persona que está siendo entrevistada?\n",
      "A: Monte Alegre, 612, en la colonia Monumental, en Guadalajara, Jalisco.\n",
      "\n",
      "\n",
      "Q: ¿Qué escolaridad tiene la persona que está siendo entrevistada?\n",
      "A: Preparatoria.\n",
      "\n",
      "\n",
      "Q: ¿La escolaridad de la persona que está siendo entrevistada está terminada?\n",
      "A: Sí.\n",
      "\n",
      "\n",
      "Q: ¿Tiene seguridad social la persona que está siendo entrevistada?\n",
      "A: Sí.\n",
      "\n",
      "\n",
      "Q: ¿A qué se dedica actualmente la persona entrevistada?\n",
      "A: Trabajo en una empresa de telecomunicaciones.\n",
      "\n",
      "\n",
      "Q: ¿Qué estado civil tiene la persona que está siendo entrevistada?\n",
      "A: Casada.\n",
      "\n",
      "\n",
      "Q: ¿Con qué régimen matrimonial está casada la persona que está siendo entrevistada?\n",
      "A: Bienes no acomodados.\n",
      "\n",
      "\n",
      "Q: ¿Cuáles son las características de la casa en la que vive la persona que está siendo entrevistada?\n",
      "A: La casa es prestada, tiene dos habitaciones, un living, un comedor, cocina, baño y un patio.\n",
      "\n",
      "\n",
      "Q: ¿La vivienda en la que la persona que está siendo entrevistada es compartida?\n",
      "A: Sí.\n",
      "\n",
      "\n",
      "Q: ¿Cuántas personas viven en la casa de la persona que está siendo entrevistada?\n",
      "A: Seis personas viven en la casa de la persona que está siendo entrevistada.\n",
      "\n",
      "\n",
      "Q: ¿Cuáles son las características de las personas con las que vive?\n",
      "A: \n",
      "\n",
      "\n",
      "Q: ¿Quién aporta el mayor ingreso dentro del hogar?\n",
      "A: Mi esposo.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el motivo de la atención (sé especifico)?\n",
      "A: ¿El motivo de la atención es porque me violenta física y emocionalmente?\n",
      "\n",
      "\n",
      "Q: ¿Ha tenido que ser atendida en una institución médica o por personal médico como consecuencia de un evento de violencia con la persona agresora?\n",
      "A: No.\n",
      "\n",
      "\n",
      "Q: ¿Cuál fue el último episodio de violencia? Describelo con detalle\n",
      "A: Sí, fue hace dos días. Me golpeó en la cara con una botella de plástico.\n",
      "\n",
      "\n",
      "Q: Nombre de la persona agresora\n",
      "A: Omar Alejandro Meléndez\n",
      "\n",
      "\n",
      "Q: ¿Cuál es el domicilio de Omar Alejandro? Responde calle y colonia.\n",
      "A: En la esquina está un depósito de cerveza y la fachada es beige con tinta, con un cáncer negro.\n",
      "\n",
      "\n",
      "Q: ¿Cuál es la escolaridad de Omar Alejandro? Responde con el grado de estudios\n",
      "A: Sí, es correcto.\n",
      "\n",
      "\n",
      "Q: ¿Omar Alejandro posee armas?\n",
      "A: No.\n",
      "\n",
      "\n",
      "Q: ¿Omar Alejandro tiene vinculos con el crimen organizado?\n",
      "A: NA\n",
      "\n",
      "\n",
      "Q: ¿Quiénes son tu red de apoyo?\n",
      "A: Mi mamá, mi esposo, mis hijos, mi amiga.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in questions:\n",
    "    answer = qa_chain.run(query)\n",
    "    answer = answer.split(\"\\n\")[0].strip()\n",
    "    print(\"Q:\", query)\n",
    "    print(\"A:\", answer)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7daad-fc8f-4aa1-9528-2ee943e57265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
